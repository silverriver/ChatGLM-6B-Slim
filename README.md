# ChatGLM-6B-Slim

## ä»‹ç»
ChatGLM-6B-Slimæ˜¯åœ¨ChatGLM-6Bçš„åŸºç¡€ä¸Šé€šè¿‡è£å‰ªè¯è¡¨æ„å»ºçš„ã€‚å› ä¸ºChatGLM-6Bä½¿ç”¨äº†icetkï¼Œåœ¨å…¶è¯è¡¨ä¸­ï¼Œå‰20000ä¸ªtokenæ˜¯é¢„ç•™ç»™å›¾ç‰‡çš„ï¼Œåœ¨æ–‡æœ¬æ¨¡å‹ä¸­æ²¡æœ‰ç”¨åˆ°è¿™äº›å›¾ç‰‡tokenï¼Œä½†æ˜¯åœ¨inferå’Œå¾®è°ƒçš„æ—¶å€™ï¼Œè¿™äº›tokenå¯¹åº”çš„embeddingä¾ç„¶éœ€è¦è¢«åŠ è½½ï¼Œå¹¶ä¸”åœ¨è§£ç æ¯ä¸€ä¸ªtokençš„æ—¶å€™éœ€è¦å¤šè®¡ç®—20Kä¸ªlogitsï¼Œä¼šå ç”¨ä¸å°‘æ˜¾å­˜ã€‚å› æ­¤å°†è¿™ä¸€éƒ¨åˆ†tokenè£å‰ªæ‰ä»¥èŠ‚çœæ˜¾å­˜å’Œè®¡ç®—ã€‚

é™¤äº†è¯è¡¨å¤–ï¼ŒChatGLM-6B-Slimçš„å…¶ä»–ç»“æ„ä¸ChatGLM-6Bå®Œå…¨ä¸€è‡´ï¼Œæ€§èƒ½ä¹Ÿå®Œå…¨ä¸€æ ·ï¼Œå¯ä»¥è®¤ä¸ºæ˜¯ChatGLM-6Bçš„ä¸€ä¸ªä½æ˜¾å­˜ç‰ˆç­‰ä»·å¹³æ›¿ã€‚å…¶ä½¿ç”¨æ–¹å¼å’ŒChatGLM-6Bå®Œå…¨ä¸€è‡´ï¼Œåªéœ€è¦ä¿®æ”¹åŠ è½½æ¨¡å‹å’Œtokenizeréƒ¨åˆ†çš„ä»£ç ã€‚

å…³äºChatGLM-6Bçš„æ›´å¤šè¯¦æƒ…è¯·å‚è€ƒå®˜æ–¹[repo](https://github.com/THUDM/ChatGLM-6B)

## ç¡¬ä»¶éœ€æ±‚

| **é‡åŒ–ç­‰çº§**    | **æœ€ä½ GPU æ˜¾å­˜** |
| -------------- | ----------------- |
| FP16ï¼ˆæ— é‡åŒ–ï¼‰   | 13 GB             |
| INT8           | 10 GB              |
| INT4           | 6 GB               |

## ä½¿ç”¨æ–¹å¼

### ç¯å¢ƒå®‰è£…

ä½¿ç”¨ pip å®‰è£…ä¾èµ–ï¼š`pip install -r requirements.txt`ï¼Œå…¶ä¸­ `transformers` åº“ç‰ˆæœ¬æ¨èä¸º `4.26.1`ï¼Œä½†ç†è®ºä¸Šä¸ä½äº `4.23.1` å³å¯ã€‚

### ä»£ç è°ƒç”¨ 

å¯ä»¥é€šè¿‡å¦‚ä¸‹ä»£ç è°ƒç”¨ ChatGLM-6B-Slim æ¨¡å‹æ¥ç”Ÿæˆå¯¹è¯ï¼š

```python
>>> from transformers import AutoTokenizer, AutoModel
>>> tokenizer = AutoTokenizer.from_pretrained("silver/chatglm-6b-slim", trust_remote_code=True)
>>> model = AutoModel.from_pretrained("silver/chatglm-6b-slim", trust_remote_code=True).half().cuda()
>>> response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
>>> print(response)
ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
>>> response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", history=history)
>>> print(response)
æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:

1. åˆ¶å®šè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨:ä¿æŒè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨å¯ä»¥å¸®åŠ©ä½ å»ºç«‹å¥åº·çš„ç¡çœ ä¹ æƒ¯,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚å°½é‡åœ¨æ¯å¤©çš„ç›¸åŒæ—¶é—´ä¸ŠåºŠ,å¹¶åœ¨åŒä¸€æ—¶é—´èµ·åºŠã€‚
2. åˆ›é€ ä¸€ä¸ªèˆ’é€‚çš„ç¡çœ ç¯å¢ƒ:ç¡®ä¿ç¡çœ ç¯å¢ƒèˆ’é€‚,å®‰é™,é»‘æš—ä¸”æ¸©åº¦é€‚å®œã€‚å¯ä»¥ä½¿ç”¨èˆ’é€‚çš„åºŠä¸Šç”¨å“,å¹¶ä¿æŒæˆ¿é—´é€šé£ã€‚
3. æ”¾æ¾èº«å¿ƒ:åœ¨ç¡å‰åšäº›æ”¾æ¾çš„æ´»åŠ¨,ä¾‹å¦‚æ³¡ä¸ªçƒ­æ°´æ¾¡,å¬äº›è½»æŸ”çš„éŸ³ä¹,é˜…è¯»ä¸€äº›æœ‰è¶£çš„ä¹¦ç±ç­‰,æœ‰åŠ©äºç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚
4. é¿å…é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™:å’–å•¡å› æ˜¯ä¸€ç§åˆºæ¿€æ€§ç‰©è´¨,ä¼šå½±å“ä½ çš„ç¡çœ è´¨é‡ã€‚å°½é‡é¿å…åœ¨ç¡å‰é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™,ä¾‹å¦‚å’–å•¡,èŒ¶å’Œå¯ä¹ã€‚
5. é¿å…åœ¨åºŠä¸Šåšä¸ç¡çœ æ— å…³çš„äº‹æƒ…:åœ¨åºŠä¸Šåšäº›ä¸ç¡çœ æ— å…³çš„äº‹æƒ…,ä¾‹å¦‚çœ‹ç”µå½±,ç©æ¸¸æˆæˆ–å·¥ä½œç­‰,å¯èƒ½ä¼šå¹²æ‰°ä½ çš„ç¡çœ ã€‚
6. å°è¯•å‘¼å¸æŠ€å·§:æ·±å‘¼å¸æ˜¯ä¸€ç§æ”¾æ¾æŠ€å·§,å¯ä»¥å¸®åŠ©ä½ ç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚è¯•ç€æ…¢æ…¢å¸æ°”,ä¿æŒå‡ ç§’é’Ÿ,ç„¶åç¼“æ…¢å‘¼æ°”ã€‚

å¦‚æœè¿™äº›æ–¹æ³•æ— æ³•å¸®åŠ©ä½ å…¥ç¡,ä½ å¯ä»¥è€ƒè™‘å’¨è¯¢åŒ»ç”Ÿæˆ–ç¡çœ ä¸“å®¶,å¯»æ±‚è¿›ä¸€æ­¥çš„å»ºè®®ã€‚
```

ChatGLM-6B-Slimå®Œæ•´çš„æ¨¡å‹å®ç°å¯ä»¥åœ¨ [è¿™é‡Œ](https://huggingface.co/silver/chatglm-6b-slim) æŸ¥çœ‹ï¼Œç›¸æ¯”ChatGLM-6Båªä¿®æ”¹äº†å’Œtokenizeræœ‰å…³çš„éƒ¨åˆ†ã€‚
å…¶ä»–ä½¿ç”¨æ–¹æ³•ï¼ˆdemoï¼Œé‡åŒ–ç­‰ï¼‰å’ŒChatGLM-6Bå®˜æ–¹Repoå®Œå…¨ä¸€è‡´ã€‚

## å¼•ç”¨

å¦‚æœä½ è§‰å¾—è¿™ä¸ªå·¥ä½œæœ‰å¸®åŠ©çš„è¯ï¼Œè¯·è€ƒè™‘å¼•ç”¨ä¸‹åˆ—ChatGLM-6Bå®˜æ–¹å›¢é˜Ÿçš„è®ºæ–‡

```
@inproceedings{
  zeng2023glm-130b,
  title={{GLM}-130B: An Open Bilingual Pre-trained Model},
  author={Aohan Zeng and Xiao Liu and Zhengxiao Du and Zihan Wang and Hanyu Lai and Ming Ding and Zhuoyi Yang and Yifan Xu and Wendi Zheng and Xiao Xia and Weng Lam Tam and Zixuan Ma and Yufei Xue and Jidong Zhai and Wenguang Chen and Zhiyuan Liu and Peng Zhang and Yuxiao Dong and Jie Tang},
  booktitle={The Eleventh International Conference on Learning Representations (ICLR)},
  year={2023},
  url={https://openreview.net/forum?id=-Aw0rrrPUF}
}
```
```
@inproceedings{du2022glm,
  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={320--335},
  year={2022}
}
```
